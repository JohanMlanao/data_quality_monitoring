# Data Quality Monitoring 

## Overview

This project simulates a real-world **data quality monitoring workflow** using a complete data pipeline. The pipeline automates data extraction, transformation, and visualization, providing insights into data quality issues.

## Live Demo

Check out the interactive Streamlit dashboard here:  
[Streamlit app](https://data-quality-monitoring-2025.streamlit.app/)

## Architecture

The pipeline consists of the following steps:

### 1. Simulated Company Database (FastAPI)
- A FastAPI backend mimics a real application exposing data via an API.

### 2. Data Extraction (Python + Pandas)
- Python scripts use `requests` and `pandas` to fetch and structure data.

### 3. Transformation and Storage (Pandas)
- Data is cleaned and stored efficiently using **Parquet format**

### 4. ETL Automation (Airflow)
- Apache Airflow orchestrates the Extract-Transform-Load process for automation.

### 5. Database Creation (DuckDB)
- Parquet data is loaded into DuckDB for efficient queryinq.

### 6. Interactive Visualization (Streamlit App)
- A Streamlit dashboard provides insights into data quality via dynamic charts.

âš ï¸ Due to slow response times from Render, manual extraction may be used during development.

---

## Tech Stack

- **FastAPI** â€” Simulated data source API  
- **Python & Pandas** â€” Data extraction, transformation, and storage  
- **Apache Airflow** â€” ETL orchestration and scheduling  
- **DuckDB** â€” Analytical database engine  
- **Streamlit** â€” Interactive dashboard for data quality visualization  

---

## Getting Started

### Prerequisites

- Python 3.8+  
- Docker (optional, for running Airflow and FastAPI)  
- `pip` package manager  

---

## ğŸ› ï¸ Installation

### 1. Clone the repository:  
   ```bash
   git clone https://github.com/yourusername/data_quality_monitoring.git
   cd data_quality_monitoring
   ```
   
### 2. Set up a virtual environment (recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the FastAPI simulated company database

```bash
uvicorn app:app --reload
```

### 5. Start AirFlow to automate ETL workflows
(Refer to airflow/README.MD or documentation)

### 6. Launch Streamlit app for interactive visualization
```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```bash
data_quality_monitoring/
â”€â”€ src/                        # Streamlit app source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                  # FastAPI app entrypoint
â”‚   â”œâ”€â”€ sensor.py               # Sensor class
â”‚   â””â”€â”€ store.py                # Store class
â”œâ”€â”€ app/                        # Streamlit app source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                  # Main Streamlit app entrypoint
â”‚   â”œâ”€â”€ homepage.py             # Homepage and navigation components
â”‚   â”œâ”€â”€ visualisation.py        # Visualization components
â”‚   â””â”€â”€ design.py               # Styling and UI design components
â”‚
â”œâ”€â”€ etl/                        # ETL related scripts and workflows
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ automated_etl.py        # Automated ETL orchestration (e.g. Airflow DAGs)
â”‚   â”œâ”€â”€ extract_data.py         # Data extraction logic
â”‚   â”œâ”€â”€ manual_extract_data.py  # Manual extraction utilities
â”‚   â”œâ”€â”€ transform_data.py       # Data transformation logic
â”‚   â””â”€â”€ manual_transform_data.py # Manual transformation utilities
â”‚
â”œâ”€â”€ data/                       # Data storage folder
â”‚   â”œâ”€â”€ raw/                    # Raw data files (e.g. from API)
â”‚   â”œâ”€â”€ processed/              # Cleaned/transformed data (Parquet, CSV, etc.)
â”‚   â”œâ”€â”€ data.duckdb
â”‚
â”œâ”€â”€ tests/                      # Test suites
â”‚   â”œâ”€â”€ test_extract_data.py
â”‚   â”œâ”€â”€ test_manual_extract_data.py
â”‚   â”œâ”€â”€ test_store.py
â”‚   â”œâ”€â”€ test_transform_data.py
â”‚   â””â”€â”€ test_visit_sensor.py      
â”‚
â”œâ”€â”€ .streamlit/ 
â”‚   â””â”€â”€ config.toml                # Streamlit config files
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ .gitignore                  # Git ignore rules
â””â”€â”€ venv/                       # Virtual environment (optional - usually gitignore)
```

---

## ğŸ’¡ Contributing

Contributions are welcome! Feel free to open issues or pull requests if you want to add new exercises, fix bugs, or improve functionality.

---